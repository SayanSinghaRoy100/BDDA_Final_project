{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python spark library\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.appName('nlp').getOrCreate()\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('nlpML').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= spark.read.csv('Corona_NLP_train.csv', header = True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|            UserName|          ScreenName|            Location|             TweetAt|       OriginalTweet|Sentiment|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|                3799|               48751|              London|          16-03-2020|@MeNyrbie @Phil_G...|  Neutral|\n",
      "|                3800|               48752|                  UK|          16-03-2020|advice Talk to yo...| Positive|\n",
      "|                3801|               48753|           Vagabonds|          16-03-2020|Coronavirus Austr...| Positive|\n",
      "|                3802|               48754|                null|          16-03-2020|My food stock is ...|     null|\n",
      "|              PLEASE|         don't panic| THERE WILL BE EN...|                null|                null|     null|\n",
      "|           Stay calm|          stay safe.|                null|                null|                null|     null|\n",
      "|#COVID19france #C...|            Positive|                null|                null|                null|     null|\n",
      "|                3803|               48755|                null|          16-03-2020|Me, ready to go a...|     null|\n",
      "|Not because I'm p...| but because my f...|          but please| don't panic. It ...|                null|     null|\n",
      "|#CoronavirusFranc...|  Extremely Negative|                null|                null|                null|     null|\n",
      "|                3804|               48756|ÜT: 36.319708,-82...|          16-03-2020|As news of the re...| Positive|\n",
      "|                3805|               48757|35.926541,-78.753267|          16-03-2020|\"Cashier at groce...| Positive|\n",
      "|                3806|               48758|             Austria|          16-03-2020|Was at the superm...|     null|\n",
      "|#toiletpapercrisi...|             Neutral|                null|                null|                null|     null|\n",
      "|                3807|               48759|     Atlanta, GA USA|          16-03-2020|Due to COVID-19 o...| Positive|\n",
      "|                3808|               48760|    BHAVNAGAR,GUJRAT|          16-03-2020|For corona preven...| Negative|\n",
      "|                3809|               48761|      Makati, Manila|          16-03-2020|All month there h...|  Neutral|\n",
      "|                3810|               48762|Pitt Meadows, BC,...|          16-03-2020|Due to the Covid-...|     null|\n",
      "|The wait time may...| particularly bee...|                null|                null|                null|     null|\n",
      "|We thank you for ...|  Extremely Positive|                null|                null|                null|     null|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.withColumn('length', length(data['OriginalTweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+------+\n",
      "|            UserName|          ScreenName|            Location|             TweetAt|       OriginalTweet|Sentiment|length|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+------+\n",
      "|                3799|               48751|              London|          16-03-2020|@MeNyrbie @Phil_G...|  Neutral|   111|\n",
      "|                3800|               48752|                  UK|          16-03-2020|advice Talk to yo...| Positive|   237|\n",
      "|                3801|               48753|           Vagabonds|          16-03-2020|Coronavirus Austr...| Positive|   131|\n",
      "|                3802|               48754|                null|          16-03-2020|My food stock is ...|     null|    51|\n",
      "|              PLEASE|         don't panic| THERE WILL BE EN...|                null|                null|     null|  null|\n",
      "|           Stay calm|          stay safe.|                null|                null|                null|     null|  null|\n",
      "|#COVID19france #C...|            Positive|                null|                null|                null|     null|  null|\n",
      "|                3803|               48755|                null|          16-03-2020|Me, ready to go a...|     null|    60|\n",
      "|Not because I'm p...| but because my f...|          but please| don't panic. It ...|                null|     null|  null|\n",
      "|#CoronavirusFranc...|  Extremely Negative|                null|                null|                null|     null|  null|\n",
      "|                3804|               48756|ÜT: 36.319708,-82...|          16-03-2020|As news of the re...| Positive|   249|\n",
      "|                3805|               48757|35.926541,-78.753267|          16-03-2020|\"Cashier at groce...| Positive|   184|\n",
      "|                3806|               48758|             Austria|          16-03-2020|Was at the superm...|     null|    61|\n",
      "|#toiletpapercrisi...|             Neutral|                null|                null|                null|     null|  null|\n",
      "|                3807|               48759|     Atlanta, GA USA|          16-03-2020|Due to COVID-19 o...| Positive|   280|\n",
      "|                3808|               48760|    BHAVNAGAR,GUJRAT|          16-03-2020|For corona preven...| Negative|   267|\n",
      "|                3809|               48761|      Makati, Manila|          16-03-2020|All month there h...|  Neutral|   276|\n",
      "|                3810|               48762|Pitt Meadows, BC,...|          16-03-2020|Due to the Covid-...|     null|    79|\n",
      "|The wait time may...| particularly bee...|                null|                null|                null|     null|  null|\n",
      "|We thank you for ...|  Extremely Positive|                null|                null|                null|     null|  null|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|           Sentiment|avg(length)|\n",
      "+--------------------+-----------+\n",
      "|    online education|      150.0|\n",
      "| potatoes &amp; v...|       23.0|\n",
      "| only a few preli...|       10.0|\n",
      "| Vaccines and Tre...|       41.0|\n",
      "|              #virus|       12.0|\n",
      "| consumer and mor...|        5.0|\n",
      "|\"\" as shoppers ac...|       63.0|\n",
      "|      Mumbai or Pune|       26.0|\n",
      "|000 tests for COV...|      144.0|\n",
      "| spot the scams\"\"...|       53.0|\n",
      "| claiming the new...|       63.0|\n",
      "| we are confronte...|       54.0|\n",
      "| they should add ...|      247.0|\n",
      "| as lower oil pri...|       72.0|\n",
      "|  closed the borders|      162.0|\n",
      "| IFB vice preside...|      227.0|\n",
      "|            teachers|       28.0|\n",
      "| but we have to a...|      142.0|\n",
      "| the company prod...|       34.0|\n",
      "| a stock market n...|       51.0|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupby('Sentiment').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
    "\n",
    "tokenizer=Tokenizer(inputCol=\"OriginalTweet\", outputCol=\"token_OriginalTweet\")\n",
    "stopremove=StopWordsRemover(inputCol=\"token_OriginalTweet\", outputCol=\"stop_tokens\")\n",
    "count_vec=CountVectorizer(inputCol=\"stop_tokens\", outputCol=\"c_vec\")\n",
    "idf=IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n",
    "\n",
    "# we also need to convert our labels in numbers\n",
    "Netural_Positive_Negative_null_to_num = StringIndexer(inputCol=\"Sentiment\", outputCol='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up = VectorAssembler(inputCols=['tf_idf','length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, DecisionTreeClassifier\n",
    "\n",
    "nb=NaiveBayes()\n",
    "rf=RandomForestClassifier(numTrees=200)\n",
    "dtc=DecisionTreeClassifier(maxDepth=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline= Pipeline(stages=[Netural_Positive_Negative_null_to_num, tokenizer, stopremove,count_vec, idf,clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner=data_prep_pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data=cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data=clean_data.select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, testing)=clean_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predictor=dtc.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results=sentiment_predictor.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_eval=MulticlassClassificationEvaluator()\n",
    "acc=acc_eval.evaluate(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy of the model is::\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
